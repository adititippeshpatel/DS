{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## nltk means natural language tool kit which is a library for working with human language data\npunkt is used to split text into sentences or words","metadata":{}},{"cell_type":"code","source":"import nltk \nnltk.download('punkt')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:23:31.130815Z","iopub.execute_input":"2025-05-05T13:23:31.131144Z","iopub.status.idle":"2025-05-05T13:23:32.860552Z","shell.execute_reply.started":"2025-05-05T13:23:31.131118Z","shell.execute_reply":"2025-05-05T13:23:32.859184Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":" from nltk import word_tokenize, sent_tokenize     ## imports two functions\n sent = \"Sachin is considered to be one of the greatest cricket players. Virat is the captain of the Indian cricket team\"\n print(word_tokenize(sent))  ## splits sentences into words\n print(sent_tokenize(sent))  ## splits the string into sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:23:35.644142Z","iopub.execute_input":"2025-05-05T13:23:35.644644Z","iopub.status.idle":"2025-05-05T13:23:35.685069Z","shell.execute_reply.started":"2025-05-05T13:23:35.644619Z","shell.execute_reply":"2025-05-05T13:23:35.684052Z"}},"outputs":[{"name":"stdout","text":"['Sachin', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', 'greatest', 'cricket', 'players', '.', 'Virat', 'is', 'the', 'captain', 'of', 'the', 'Indian', 'cricket', 'team']\n['Sachin is considered to be one of the greatest cricket players.', 'Virat is the captain of the Indian cricket team']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":" from nltk.corpus import stopwords  ## import common stop words from nltk\n import nltk\n nltk.download('stopwords')     ## 179 stop words are downloaded\n stop_words = stopwords.words('english')\n print(stop_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:23:38.123865Z","iopub.execute_input":"2025-05-05T13:23:38.124179Z","iopub.status.idle":"2025-05-05T13:23:38.146545Z","shell.execute_reply.started":"2025-05-05T13:23:38.124156Z","shell.execute_reply":"2025-05-05T13:23:38.145459Z"}},"outputs":[{"name":"stdout","text":"['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":" token = word_tokenize(sent) ## splits the sentences into words\n cleaned_token = []         ## creates a list to store the words after stop words removal\n for word in token:\n  if word not in stop_words:  ## keeps the word which is not in stop word\n    cleaned_token.append(word)\n print(\"This is the unclean version : \",token)\n print(\"This is the cleaned version : \",cleaned_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:23:40.734043Z","iopub.execute_input":"2025-05-05T13:23:40.734372Z","iopub.status.idle":"2025-05-05T13:23:40.740655Z","shell.execute_reply.started":"2025-05-05T13:23:40.734346Z","shell.execute_reply":"2025-05-05T13:23:40.739719Z"}},"outputs":[{"name":"stdout","text":"This is the unclean version :  ['Sachin', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', 'greatest', 'cricket', 'players', '.', 'Virat', 'is', 'the', 'captain', 'of', 'the', 'Indian', 'cricket', 'team']\nThis is the cleaned version :  ['Sachin', 'considered', 'one', 'greatest', 'cricket', 'players', '.', 'Virat', 'captain', 'Indian', 'cricket', 'team']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":" words = [cleaned_token.lower() for cleaned_token in cleaned_token if  ## lower converts into lower case\ncleaned_token.isalpha()] ## alpha keeps the alphanumeric words and ignores the punctuation\n print(words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:23:48.447840Z","iopub.execute_input":"2025-05-05T13:23:48.448151Z","iopub.status.idle":"2025-05-05T13:23:48.453604Z","shell.execute_reply.started":"2025-05-05T13:23:48.448128Z","shell.execute_reply":"2025-05-05T13:23:48.452465Z"}},"outputs":[{"name":"stdout","text":"['sachin', 'considered', 'one', 'greatest', 'cricket', 'players', 'virat', 'captain', 'indian', 'cricket', 'team']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## stemming reduces the words into base form like running is converted to run","metadata":{}},{"cell_type":"code","source":" from nltk.stem import PorterStemmer  ## porter stemmer is a popular stemming algorithm\n stemmer = PorterStemmer()            ## stemmer object is created\n port_stemmer_output = [stemmer.stem(words) for words in words]\n print(port_stemmer_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T05:52:04.632865Z","iopub.execute_input":"2025-05-03T05:52:04.633166Z","iopub.status.idle":"2025-05-03T05:52:04.639033Z","shell.execute_reply.started":"2025-05-03T05:52:04.633143Z","shell.execute_reply":"2025-05-03T05:52:04.638124Z"}},"outputs":[{"name":"stdout","text":"['sachin', 'consid', 'one', 'greatest', 'cricket', 'player', 'virat', 'captain', 'indian', 'cricket', 'team']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## lemmatiztion is same as stemming but more accurate","metadata":{}},{"cell_type":"code","source":" from nltk.stem import WordNetLemmatizer\n nltk.download('wordnet')\n lemmatizer = WordNetLemmatizer()\n lemmatizer_output = [lemmatizer.lemmatize(words) for words in words]\n print(lemmatizer_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T05:52:24.130544Z","iopub.execute_input":"2025-05-03T05:52:24.130884Z","iopub.status.idle":"2025-05-03T05:52:27.368349Z","shell.execute_reply.started":"2025-05-03T05:52:24.130860Z","shell.execute_reply":"2025-05-03T05:52:27.367539Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"['sachin', 'considered', 'one', 'greatest', 'cricket', 'player', 'virat', 'captain', 'indian', 'cricket', 'team']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## perform pos tagging i.e part of speech tagging i.e whether it is verb, noun,pronoun, etc","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords\n\n# Download required resources\nnltk.download('punkt')  # for word_tokenize\nnltk.download('stopwords')  # for stopword removal\nnltk.download('averaged_perceptron_tagger_eng')  ## model required to pos of each word\n\n\n# Set of English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Tokenize the sentence\ntoken = word_tokenize(sent)\n\n# Remove stopwords\ncleaned_token = [word for word in token if word.lower() not in stop_words]\n\n# POS tagging\ntagged = pos_tag(cleaned_token, lang='eng')\n\n# Output the result\nprint(\"POS Tagged Tokens:\")\nprint(tagged)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T05:56:42.011567Z","iopub.execute_input":"2025-05-03T05:56:42.012032Z","iopub.status.idle":"2025-05-03T05:56:42.700164Z","shell.execute_reply.started":"2025-05-03T05:56:42.012004Z","shell.execute_reply":"2025-05-03T05:56:42.699028Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n","output_type":"stream"},{"name":"stdout","text":"POS Tagged Tokens:\n[('Sachin', 'NNP'), ('considered', 'VBD'), ('one', 'CD'), ('greatest', 'JJS'), ('cricket', 'NN'), ('players', 'NNS'), ('.', '.'), ('Virat', 'NNP'), ('captain', 'NN'), ('Indian', 'JJ'), ('cricket', 'NN'), ('team', 'NN')]\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## tf means term frequency i.e no. of times the words appear in a document\nidf means inverse document fre. which measures how importnat the word is in the entire set of documents\n\nat last tf is multiplied with idf\n\nmore unique words have higher tf-idf","metadata":{}},{"cell_type":"code","source":" from sklearn.feature_extraction.text import TfidfVectorizer\n from sklearn.metrics.pairwise import cosine_similarity\n import pandas as pd\n docs = [ \"Sachin is considered to be one of the greatest cricket players\",\n \"Federer is considered one of the greatest tennis players\",\n \"Nadal is considered one of the greatest tennis players\",\n \"Virat is the captain of the Indian cricket team\"]\n vectorizer = TfidfVectorizer(analyzer = \"word\", norm = None , use_idf \n= True , smooth_idf=True)\n Mat = vectorizer.fit(docs)\n print(Mat.vocabulary_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:43:21.364633Z","iopub.execute_input":"2025-05-05T13:43:21.364923Z","iopub.status.idle":"2025-05-05T13:43:21.809765Z","shell.execute_reply.started":"2025-05-05T13:43:21.364902Z","shell.execute_reply":"2025-05-05T13:43:21.808950Z"}},"outputs":[{"name":"stdout","text":"{'sachin': 12, 'is': 7, 'considered': 2, 'to': 16, 'be': 0, 'one': 10, 'of': 9, 'the': 15, 'greatest': 5, 'cricket': 3, 'players': 11, 'federer': 4, 'tennis': 14, 'nadal': 8, 'virat': 17, 'captain': 1, 'indian': 6, 'team': 13}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 'sachin' is at index 15 in the vector.\n\n'cricket' is at index 3.","metadata":{}},{"cell_type":"code","source":" tfidfMat = vectorizer.fit_transform(docs)\n print(tfidfMat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:43:25.627685Z","iopub.execute_input":"2025-05-05T13:43:25.628102Z","iopub.status.idle":"2025-05-05T13:43:25.638375Z","shell.execute_reply.started":"2025-05-05T13:43:25.628079Z","shell.execute_reply":"2025-05-05T13:43:25.637326Z"}},"outputs":[{"name":"stdout","text":"<Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 37 stored elements and shape (4, 18)>\n  Coords\tValues\n  (0, 11)\t1.2231435513142097\n  (0, 3)\t1.5108256237659907\n  (0, 5)\t1.2231435513142097\n  (0, 15)\t1.0\n  (0, 9)\t1.0\n  (0, 10)\t1.2231435513142097\n  (0, 0)\t1.916290731874155\n  (0, 16)\t1.916290731874155\n  (0, 2)\t1.2231435513142097\n  (0, 7)\t1.0\n  (0, 12)\t1.916290731874155\n  (1, 14)\t1.5108256237659907\n  (1, 4)\t1.916290731874155\n  (1, 11)\t1.2231435513142097\n  (1, 5)\t1.2231435513142097\n  (1, 15)\t1.0\n  (1, 9)\t1.0\n  (1, 10)\t1.2231435513142097\n  (1, 2)\t1.2231435513142097\n  (1, 7)\t1.0\n  (2, 8)\t1.916290731874155\n  (2, 14)\t1.5108256237659907\n  (2, 11)\t1.2231435513142097\n  (2, 5)\t1.2231435513142097\n  (2, 15)\t1.0\n  (2, 9)\t1.0\n  (2, 10)\t1.2231435513142097\n  (2, 2)\t1.2231435513142097\n  (2, 7)\t1.0\n  (3, 13)\t1.916290731874155\n  (3, 6)\t1.916290731874155\n  (3, 1)\t1.916290731874155\n  (3, 17)\t1.916290731874155\n  (3, 3)\t1.5108256237659907\n  (3, 15)\t2.0\n  (3, 9)\t1.0\n  (3, 7)\t1.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## (3, 12) refers to the word at index 12 having a TF-IDF score of 0.447213 in the fourth document.\n\n","metadata":{}},{"cell_type":"markdown","source":"## all the important words are printed","metadata":{}},{"cell_type":"code","source":"features_names = vectorizer.get_feature_names_out()\nprint(features_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:43:29.133794Z","iopub.execute_input":"2025-05-05T13:43:29.134075Z","iopub.status.idle":"2025-05-05T13:43:29.139072Z","shell.execute_reply.started":"2025-05-05T13:43:29.134056Z","shell.execute_reply":"2025-05-05T13:43:29.138198Z"}},"outputs":[{"name":"stdout","text":"['be' 'captain' 'considered' 'cricket' 'federer' 'greatest' 'indian' 'is'\n 'nadal' 'of' 'one' 'players' 'sachin' 'team' 'tennis' 'the' 'to' 'virat']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## converted into dense matrix\neach row represents document and values are the tf idf values","metadata":{}},{"cell_type":"code","source":" dense = tfidfMat.todense()\n denselist = dense.tolist()\n df = pd.DataFrame(denselist , columns = features_names)\n df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:43:31.878818Z","iopub.execute_input":"2025-05-05T13:43:31.879090Z","iopub.status.idle":"2025-05-05T13:43:31.914810Z","shell.execute_reply.started":"2025-05-05T13:43:31.879072Z","shell.execute_reply":"2025-05-05T13:43:31.913816Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         be   captain  considered   cricket   federer  greatest    indian  \\\n0  1.916291  0.000000    1.223144  1.510826  0.000000  1.223144  0.000000   \n1  0.000000  0.000000    1.223144  0.000000  1.916291  1.223144  0.000000   \n2  0.000000  0.000000    1.223144  0.000000  0.000000  1.223144  0.000000   \n3  0.000000  1.916291    0.000000  1.510826  0.000000  0.000000  1.916291   \n\n    is     nadal   of       one   players    sachin      team    tennis  the  \\\n0  1.0  0.000000  1.0  1.223144  1.223144  1.916291  0.000000  0.000000  1.0   \n1  1.0  0.000000  1.0  1.223144  1.223144  0.000000  0.000000  1.510826  1.0   \n2  1.0  1.916291  1.0  1.223144  1.223144  0.000000  0.000000  1.510826  1.0   \n3  1.0  0.000000  1.0  0.000000  0.000000  0.000000  1.916291  0.000000  2.0   \n\n         to     virat  \n0  1.916291  0.000000  \n1  0.000000  0.000000  \n2  0.000000  0.000000  \n3  0.000000  1.916291  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>be</th>\n      <th>captain</th>\n      <th>considered</th>\n      <th>cricket</th>\n      <th>federer</th>\n      <th>greatest</th>\n      <th>indian</th>\n      <th>is</th>\n      <th>nadal</th>\n      <th>of</th>\n      <th>one</th>\n      <th>players</th>\n      <th>sachin</th>\n      <th>team</th>\n      <th>tennis</th>\n      <th>the</th>\n      <th>to</th>\n      <th>virat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>1.510826</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.223144</td>\n      <td>1.223144</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.223144</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.510826</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>1.916291</td>\n      <td>1.0</td>\n      <td>1.223144</td>\n      <td>1.223144</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.510826</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>1.510826</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n      <td>0.000000</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>1.916291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"features_names = sorted(vectorizer.get_feature_names_out()) ## sorts in alphabetical order\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:44:27.016848Z","iopub.execute_input":"2025-05-05T13:44:27.017125Z","iopub.status.idle":"2025-05-05T13:44:27.022519Z","shell.execute_reply.started":"2025-05-05T13:44:27.017106Z","shell.execute_reply":"2025-05-05T13:44:27.021549Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":" docList = ['Doc 1','Doc 2','Doc 3','Doc 4']\n skDocsIfIdfdf = pd.DataFrame(tfidfMat.todense(),index = \nsorted(docList), columns=features_names)\n print(skDocsIfIdfdf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T06:01:39.851519Z","iopub.execute_input":"2025-05-03T06:01:39.852775Z","iopub.status.idle":"2025-05-03T06:01:39.866962Z","shell.execute_reply.started":"2025-05-03T06:01:39.852743Z","shell.execute_reply":"2025-05-03T06:01:39.865774Z"}},"outputs":[{"name":"stdout","text":"             be   captain  considered   cricket   federer  greatest    indian  \\\nDoc 1  1.916291  0.000000    1.223144  1.510826  0.000000  1.223144  0.000000   \nDoc 2  0.000000  0.000000    1.223144  0.000000  1.916291  1.223144  0.000000   \nDoc 3  0.000000  0.000000    1.223144  0.000000  0.000000  1.223144  0.000000   \nDoc 4  0.000000  1.916291    0.000000  1.510826  0.000000  0.000000  1.916291   \n\n        is     nadal   of       one   players    sachin      team    tennis  \\\nDoc 1  1.0  0.000000  1.0  1.223144  1.223144  1.916291  0.000000  0.000000   \nDoc 2  1.0  0.000000  1.0  1.223144  1.223144  0.000000  0.000000  1.510826   \nDoc 3  1.0  1.916291  1.0  1.223144  1.223144  0.000000  0.000000  1.510826   \nDoc 4  1.0  0.000000  1.0  0.000000  0.000000  0.000000  1.916291  0.000000   \n\n       the        to     virat  \nDoc 1  1.0  1.916291  0.000000  \nDoc 2  1.0  0.000000  0.000000  \nDoc 3  1.0  0.000000  0.000000  \nDoc 4  2.0  0.000000  1.916291  \n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## This calculates the cosine similarity between each pair of documents in the TF-IDF matrix.\n\nThe cosine similarity is a measure of how similar two documents are, based on their TF-IDF vectors. The values range from 0 to 1, where:\n\n1 means the documents are identical in terms of word usage.\n\n0 means the documents are completely different.","metadata":{}},{"cell_type":"code","source":"csim = cosine_similarity(tfidfMat,tfidfMat)\ncsimDf = pd.DataFrame(csim,index=sorted(docList),columns=sorted(docList))\nprint(csimDf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T06:02:14.818734Z","iopub.execute_input":"2025-05-03T06:02:14.819116Z","iopub.status.idle":"2025-05-03T06:02:14.829455Z","shell.execute_reply.started":"2025-05-03T06:02:14.819092Z","shell.execute_reply":"2025-05-03T06:02:14.828279Z"}},"outputs":[{"name":"stdout","text":"          Doc 1     Doc 2     Doc 3     Doc 4\nDoc 1  1.000000  0.492416  0.492416  0.277687\nDoc 2  0.492416  1.000000  0.754190  0.215926\nDoc 3  0.492416  0.754190  1.000000  0.215926\nDoc 4  0.277687  0.215926  0.215926  1.000000\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}